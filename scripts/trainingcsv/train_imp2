import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.metrics import r2_score, mean_absolute_error
from xgboost import XGBRegressor
import joblib

# === 1. Load and Clean Data ===
df = pd.read_csv("sql/parsed_data.csv")

def clean_col(col):
    col = col.strip().lower()
    col = re.sub(r'[():]', '', col)
    col = re.sub(r'\s+', '_', col)
    return col

df.columns = [clean_col(col) for col in df.columns]

# === 2. Identify Target Column ===
target_col_candidates = [col for col in df.columns if "comfort" in col and "index" in col]
assert len(target_col_candidates) == 1, "❌ Could not uniquely identify comfort index column."
target_col = target_col_candidates[0]

# === 3. Select ONLY the Features of Interest ===
# Replace these with your actual cleaned column names if they differ
important_features = ['RT60_norm', 'SPL_norm', 'Absorption_norm']
# Optionally include a categorical context (uncomment if you want to keep it)
# important_features += ['zone']   # or the actual zone column name

X = df[important_features].copy()
y = df[target_col]

# === 4. Detect Feature Types ===
categorical = X.select_dtypes(include=["object"]).columns.tolist()
numeric = X.select_dtypes(exclude=["object"]).columns.tolist()

# === 5. Preprocessing Pipelines ===
num_transformer = Pipeline([
    ("imputer", KNNImputer(n_neighbors=3)),
    ("scaler", StandardScaler())
])

cat_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

preprocessor = ColumnTransformer([
    ("num", num_transformer, numeric),
    ("cat", cat_transformer, categorical)
])

# === 6. Define Model & Hyperparameter Grid ===
xgb = XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1, verbosity=1)
param_grid = {
    'regressor__n_estimators': [100, 200],
    'regressor__max_depth': [3, 6],
    'regressor__learning_rate': [0.05, 0.1, 0.2]
}

pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("regressor", xgb)
])

# === 7. Train/Test Split & GridSearchCV ===
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

grid = GridSearchCV(
    pipeline,
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    verbose=2,
    n_jobs=-1
)

grid.fit(X_train, y_train)

print(f"\n✅ Best parameters: {grid.best_params_}")
print(f"Best CV R²: {grid.best_score_:.4f}")

# === 8. Final Evaluation on Test Data ===
y_pred = grid.predict(X_test)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
print(f"Test R²: {r2:.3f}")
print(f"Test MAE: {mae:.3f}")

# === 9. Feature Importance ===
feature_names = []
if hasattr(grid.best_estimator_.named_steps["preprocessor"], "get_feature_names_out"):
    feature_names = grid.best_estimator_.named_steps["preprocessor"].get_feature_names_out()
else:
    feature_names = numeric + categorical

importances = grid.best_estimator_.named_steps["regressor"].feature_importances_
top_idx = np.argsort(importances)[::-1]
print("\nFeature Importances:")
for i in top_idx:
    print(f"{feature_names[i]}: {importances[i]:.4f}")

# === 10. Save Model ===
joblib.dump(grid.best_estimator_, "model/ecoform_xgb_comfort_focused3.pkl")
print("✅ Tuned model saved to model/ecoform_xgb_comfort_focused3.pkl")
